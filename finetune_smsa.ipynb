{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "finetune_smsa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdyanggara/indonlu/blob/master/finetune_smsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nfPaiUkzhyj"
      },
      "source": [
        "# Finetuning SmSA\n",
        "SmSA is a Sentiment Analysis dataset with 3 possible labels: `positive`, `negative`, and `neutral`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx5YR5-Szhyo"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABhmlCYg4tCz"
      },
      "source": [
        "import string\n",
        "import torch\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b0zBxR14vKb"
      },
      "source": [
        "import itertools\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMCfew9Y8HPy"
      },
      "source": [
        "train = pd.read_csv('./twitter_training.csv', header=None)\n",
        "test = pd.read_csv('./twitter_validation.csv', header=None)"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2jtQwnxHzmi"
      },
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFubj9hyMZEZ"
      },
      "source": [
        "train_size = len(train)*2 //3"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxkeVXQo9Gpj"
      },
      "source": [
        "valid = train.iloc[train_size:, [2,3]]\n",
        "valid.columns = ['sentiment', 'text']\n",
        "\n",
        "train = train.iloc[:train_size, [2,3]]\n",
        "train.columns = ['sentiment', 'text']\n",
        "\n",
        "\n",
        "\n",
        "test = test.iloc[:, [2,3]]\n",
        "test.columns = ['sentiment', 'text']"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6jPympB-2Md"
      },
      "source": [
        "train.to_csv(\"train.tsv\", sep=\"\\t\", index=False, header=None)\n",
        "valid.to_csv(\"valid.tsv\", sep=\"\\t\", index=False, header=None)\n",
        "test.to_csv(\"test.tsv\", sep=\"\\t\", index=False, header=None)"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhzy8gob4j8U"
      },
      "source": [
        "#####\n",
        "# Document Sentiment Prosa\n",
        "#####\n",
        "class DocumentSentimentDataset(Dataset):\n",
        "    # Static constant variable\n",
        "    LABEL2INDEX = {'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
        "    INDEX2LABEL = {0: 'Positive', 1: 'Neutral', 2: 'Negative', 3: 'Irrelevant'}\n",
        "    NUM_LABELS = 3\n",
        "    \n",
        "    def load_dataset(self, path): \n",
        "        df = pd.read_csv(path, sep='\\t', header=None)\n",
        "        df.columns = ['sentiment','text']\n",
        "        df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab])\n",
        "        return df\n",
        "    \n",
        "    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n",
        "        self.data = self.load_dataset(dataset_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.no_special_token = no_special_token\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data.loc[index,:]\n",
        "        text, sentiment = data['text'], data['sentiment']\n",
        "        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
        "        return np.array(subwords), np.array(sentiment), data['text']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)    \n",
        "        \n",
        "class DocumentSentimentDataLoader(DataLoader):\n",
        "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
        "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = self._collate_fn\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "    def _collate_fn(self, batch):\n",
        "        batch_size = len(batch)\n",
        "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
        "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "        \n",
        "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
        "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
        "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
        "        \n",
        "        seq_list = []\n",
        "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
        "            subwords = subwords[:max_seq_len]\n",
        "            subword_batch[i,:len(subwords)] = subwords\n",
        "            mask_batch[i,:len(subwords)] = 1\n",
        "            sentiment_batch[i,0] = sentiment\n",
        "            \n",
        "            seq_list.append(raw_seq)\n",
        "            \n",
        "        return subword_batch, mask_batch, sentiment_batch, seq_list\n"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qluKA94O_1"
      },
      "source": [
        "def document_sentiment_metrics_fn(list_hyp, list_label):\n",
        "    metrics = {}\n",
        "    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n",
        "    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n",
        "    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n",
        "    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n",
        "    return metrics\n"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZX1w-gd4GLl"
      },
      "source": [
        "# Forward function for sequence classification\n",
        "def forward_sequence_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
        "    # Unpack batch data\n",
        "    if len(batch_data) == 3:\n",
        "        (subword_batch, mask_batch, label_batch) = batch_data\n",
        "        token_type_batch = None\n",
        "    elif len(batch_data) == 4:\n",
        "        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n",
        "    \n",
        "    # Prepare input & label\n",
        "    subword_batch = torch.LongTensor(subword_batch)\n",
        "    mask_batch = torch.FloatTensor(mask_batch)\n",
        "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
        "    label_batch = torch.LongTensor(label_batch)\n",
        "            \n",
        "    if device == \"cuda\":\n",
        "        subword_batch = subword_batch.cuda()\n",
        "        mask_batch = mask_batch.cuda()\n",
        "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
        "        label_batch = label_batch.cuda()\n",
        "\n",
        "    # Forward model\n",
        "    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
        "    loss, logits = outputs[:2]\n",
        "    \n",
        "    # generate prediction & label list\n",
        "    list_hyp = []\n",
        "    list_label = []\n",
        "    hyp = torch.topk(logits, 1)[1]\n",
        "    for j in range(len(hyp)):\n",
        "        list_hyp.append(i2w[hyp[j].item()])\n",
        "        list_label.append(i2w[label_batch[j][0].item()])\n",
        "        \n",
        "    return loss, list_hyp, list_label\n"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBumLamtzhyp"
      },
      "source": [
        "###\n",
        "# common functions\n",
        "###\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "    \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik5lipytzhyq"
      },
      "source": [
        "# Set random seed\n",
        "set_seed(26092020)"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASK_X6L8zhyr"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsFqNJTDKAZa"
      },
      "source": [
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-mini')\n",
        "config = BertConfig.from_pretrained('prajjwal1/bert-mini')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-mini', config=config)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QgFJvYJKE20"
      },
      "source": [
        "model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0uvnIZXKFjN"
      },
      "source": [
        "count_param(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHpV9e5Nzhyu"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olf7HlYTzhyv"
      },
      "source": [
        "train_dataset_path = './train.tsv'\n",
        "valid_dataset_path = './valid.tsv'\n",
        "test_dataset_path = './test.tsv'"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx2WUg67zhyw"
      },
      "source": [
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  \n",
        "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  \n",
        "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am1Fz4pQzhyw",
        "outputId": "d72cb051-f901-414d-a83e-d19685aa5d1c"
      },
      "source": [
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
            "{0: 'Positive', 1: 'Neutral', 2: 'Negative', 3: 'Irrelevant'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "QSAsg9p0FAW2",
        "outputId": "0f294c8a-7687-4398-d00b-c01856089d67"
      },
      "source": [
        "train_dataset.data.loc[train_dataset.data.text.isna()]"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sentiment, text]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIwMPpZSFaD1"
      },
      "source": [
        " for batch in train_loader:\n",
        "   break"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9KmZh9uzhyx"
      },
      "source": [
        "# Test model on sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6yxS2rCzhyx",
        "outputId": "57a1c7d8-c233-4bef-d0da-b9a24e1ae442"
      },
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : Negative (45.507%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcTaiPNgzhyx",
        "outputId": "ebba78cb-b29e-4fb9-cd44-b002274dc55a"
      },
      "source": [
        "text = 'Budi pergi ke pondok indah mall membeli cakwe'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Budi pergi ke pondok indah mall membeli cakwe | Label : Negative (48.886%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBaalPYAzhyy",
        "outputId": "2a098df7-0672-4fb2-8ec6-bd49be15ee95"
      },
      "source": [
        "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Dasar anak sialan!! Kurang ajar!! | Label : Negative (66.920%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uslv4ZqBzhyy"
      },
      "source": [
        "# Fine Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXoH-BNszhyy"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
        "model = model.cuda()"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4cj3tWNzhyy",
        "outputId": "ec06c130-5ad7-4051-c4b1-224a23503eb0"
      },
      "source": [
        "# Train\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        " \n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    \n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]        \n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "        \n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        # metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f}\".format(total_loss/(i+1)))\n",
        "        \n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.9795 LR:0.00000300: 100%|██████████| 1542/1542 [02:02<00:00, 12.61it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.9795 ACC:0.38 F1:0.28 REC:0.34 PRE:0.34 LR:0.00000300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.9159: 100%|██████████| 771/771 [00:29<00:00, 25.75it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) VALID LOSS:0.9159 ACC:0.50 F1:0.40 REC:0.45 PRE:0.42\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 2) TRAIN LOSS:1.4813 LR:0.00000300: 100%|██████████| 1542/1542 [02:02<00:00, 12.63it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 2) TRAIN LOSS:1.4813 ACC:0.50 F1:0.40 REC:0.45 PRE:0.39 LR:0.00000300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.8566: 100%|██████████| 771/771 [00:29<00:00, 25.79it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 2) VALID LOSS:0.8566 ACC:0.55 F1:0.44 REC:0.48 PRE:0.43\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.8315 LR:0.00000300: 100%|██████████| 1542/1542 [02:02<00:00, 12.62it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.8315 ACC:0.53 F1:0.43 REC:0.48 PRE:0.41 LR:0.00000300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.8425: 100%|██████████| 771/771 [00:29<00:00, 25.87it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 3) VALID LOSS:0.8425 ACC:0.56 F1:0.44 REC:0.49 PRE:0.44\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 4) TRAIN LOSS:1.2087 LR:0.00000300: 100%|██████████| 1542/1542 [02:02<00:00, 12.58it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 4) TRAIN LOSS:1.2087 ACC:0.55 F1:0.45 REC:0.50 PRE:0.42 LR:0.00000300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.8397: 100%|██████████| 771/771 [00:29<00:00, 25.76it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 4) VALID LOSS:0.8397 ACC:0.56 F1:0.45 REC:0.49 PRE:0.44\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.9833 LR:0.00000300: 100%|██████████| 1542/1542 [02:02<00:00, 12.62it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.9833 ACC:0.56 F1:0.46 REC:0.51 PRE:0.44 LR:0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VALID LOSS:0.8385: 100%|██████████| 771/771 [00:30<00:00, 25.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) VALID LOSS:0.8385 ACC:0.57 F1:0.45 REC:0.49 PRE:0.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmVJnFCXzhyz",
        "outputId": "e39321f3-1008-4574-d52c-ec91c6b2fa23"
      },
      "source": [
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "\n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 12.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     index     label\n",
            "0        0   Neutral\n",
            "1        1   Neutral\n",
            "2        2  Negative\n",
            "3        3  Negative\n",
            "4        4   Neutral\n",
            "..     ...       ...\n",
            "995    995   Neutral\n",
            "996    996  Positive\n",
            "997    997  Positive\n",
            "998    998  Positive\n",
            "999    999   Neutral\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snIS1JWNzhy0"
      },
      "source": [
        "# Test fine-tuned model on sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAAjNx4Zzhy0",
        "outputId": "730f1e50-7f40-4b3a-9168-12a0b2fcdf01"
      },
      "source": [
        "text = '@Borderlands how do I submit a complaint? Your CEO isnt paying his staff their bonuses.'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: @Borderlands how do I submit a complaint? Your CEO isnt paying his staff their bonuses. | Label : Negative (79.191%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1SGFtEhzhy0",
        "outputId": "b4afedc9-1b5c-426d-9c71-77fe0991a8a2"
      },
      "source": [
        "text = 'Grounded almost looked pretty cool even despite the borderlands tier unfunny writing until it became...'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Grounded almost looked pretty cool even despite the borderlands tier unfunny writing until it became... | Label : Positive (81.800%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdNTfFgzhy1",
        "outputId": "02868366-16f6-4991-f253-be79235918c4"
      },
      "source": [
        "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Dasar anak sialan!! Kurang ajar!! | Label : Negative (66.920%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Gq97vRO1Vs",
        "outputId": "27ecb238-f99b-459d-f639-6c234f786a50"
      },
      "source": [
        "text = 'I hate that this easy mayhem modifier event on mayhem won’t last forever. this is the most fun i’ve ...'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I hate that this easy mayhem modifier event on mayhem won’t last forever. this is the most fun i’ve ... | Label : Positive (84.518%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JnNcM4zO8j5",
        "outputId": "74bcac7b-a00c-4b7a-bf13-64203c688b9e"
      },
      "source": [
        "text = 'WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Thank you everyone for hanging out everyone! It was fun. ...'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Thank you everyone for hanging out everyone! It was fun. ... | Label : Positive (76.446%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R6sujGaO7lD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX4d8Xguzhy1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}